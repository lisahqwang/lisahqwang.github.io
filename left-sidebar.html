<!DOCTYPE HTML>
<html>
	<head>
		<title>Lisa Huiqin Wang</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="left-sidebar is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<div id="header">

					<!-- Inner -->
						<div class="inner">
							<header>
								<h1><a href="index.html" id="logo">Lisa Huiqin Wang</a></h1>
							</header>
						</div>

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="left-sidebar.html">AI Safety</a></li>
								<li><a href="right-sidebar.html">Tech Consulting</a></li>
								<li><a href="no-sidebar.html">Personal Blog</a></li>
								<li>
									<a href="#">Dropdown</a>
									<ul>
										<li><a href="https://docs.google.com/spreadsheets/d/1nkLj2N0M5hG-xeODItj2e1KfNFLbQImvZJpnb1YZfgg/edit?usp=sharing">Booklog</a></li>
										<li><a href="https://dailybruin.com/author/lisa-wang">Daily Bruin</a></li>
										<li><a href="https://www.lesswrong.com/users/lisa-wang-1">LessWrong</a></li>
									</ul>
								</li>
							</ul>
						</nav>

				</div>

			<!-- Main -->
				<div class="wrapper style1">

					<div class="container">
						<div class="row gtr-200">
							<div class="col-4 col-12-mobile" id="sidebar">
								<hr class="first" />
								<section>
									<header>
										<h3><a href="#">My three projects</a></h3>
									</header>
									<p>
										<a href = "https://docs.google.com/document/d/1BJ8NBe7rW2L8J4_L6LWtRfrxgsbCgumYjT59heF7mzY/edit?usp=share_link"> ML Scale Map </a>
									</p>
									<p>
										<a href = "https://docs.google.com/document/d/1zk6aqtoHsXvW0hcxjC16GY2SXpY1gEh6QjtvtTbHOio/edit?usp=sharing"> Tech Temptations </a>
									</p>
									<p>
										<a href = "https://github.com/lisahqwang/TransformerArchToy"> Toy Transformer Architecture </a>
									<footer>
										<a href="https://docs.google.com/document/d/1yMP9i6cQQwHG1ITOtccUkuN3p9eTk1AtzOM8TP-mZVs/edit" class="button">More Projects that I want to try!</a>
									</footer>
								</section>
								<hr />
								<section>
									<header>
										<h3><a href="#">AGI Safety Sub-topics</a></h3>
									</header>
									<p>
										Ever expanding
									</p>
									<div class="row gtr-50">
										<div class="col-4">
											<a href="#" class="image fit"><img src="images/AGI.png" alt="" /></a>
										</div>
										<div class="col-8">
											<h4>Artificial General Intelligence</h4>
											<p>
												Foundation models, Meta-learning, Grokking, engineering vs. philosophy worldview
											</p>
										</div>
										<div class="col-4">
											<a href="#" class="image fit"><img src="images/RLHF.png" alt="" /></a>
										</div>
										<div class="col-8">
											<h4>Reward misspecification and instrumental convergence</h4>
											<p>
												Specification gaming, RLHF, IRL, deceptive reward hacking, instrumental convergence, failure modes.
											</p>
										</div>
										<div class="col-4">
											<a href="#" class="image fit"><img src="images/GMG.png" alt="" /></a>
										</div>
										<div class="col-8">
											<h4>Goal misgeneralisation</h4>
											<p>
												Ideal objective (wishes) vs. Design objective (blueprint) vs. Revealed objective (behavior). Inner alignment and outer alignment.  
												Mechanistic Interpretability. 
											</p>
										</div>
										
										<div class="col-4">
											<a href="#" class="image fit"><img src="images/TaskD.png" alt="" /></a>
										</div>
										
										<div class="col-8">
											<h4>Task Decomposition for Scalable Oversight</h4>
											<p>
												Iterated Amplification, training signals, Chain-of-Thought reasoning
											</p>
										</div>
										
										<div class="col-4">
											<a href="#" class="image fit"><img src="images/ScaleableOversight.png" alt="" /></a>
										</div>
										
										<div class="col-8">
											<h4>Adversarial techniques for scalable oversight </h4>
											<p>
												Red-teaming LLMs with LLMs, human preference-based reinforcement learning (HPRL), feature-level attacks
											</p>
										</div>
										
										<div class="col-4">
											<a href="#" class="image fit"><img src="images/Interpretability.png" alt="" /></a>
										</div>
										
										<div class="col-8">
											<h4>Interpretability </h4>
											<p>
												Feature visualization, circuits, ReLU, superposition, linear-classifier probing
											</p>
										</div>
										
										<div class="col-4">
											<a href="#" class="image fit"><img src="images/week 7.png" alt="" /></a>
										</div>
										
										<div class="col-8">
											<h4>AI Governance </h4>
											<p>
												AI deployment, policy, theory of change, regulation of TPU production and distribution, info hazard/security
											</p>
										</div>
									</div>
									<footer>
										<a href="https://www.agisafetyfundamentals.com/ai-alignment-curriculum" class="button">More Organized Curriculum here.</a>
									</footer>
								</section>
							</div>
							<div class="col-8 col-12-mobile imp-mobile" id="content">
								<article id="main">
									<header>
										<h2><a href="#">AI Safety</a></h2>
										<p>
											Who will solve the alignment problem?
										</p>
									</header>
									<a href="#" class="image featured"><img src="images/AI Safety work Compartmentalization.png" alt="" /></a>
									<p>
										AI Safety Communications, AI Governance, and general AIS field building tangible plans, written in a roadmap format.
									</p>
									<section>
										<header>
											<h3>Solving Alignment is difficult. Progress is chaotic. </h3>
										</header>
										<p>
											I want to be a part of the effort to solve the most pressing X-risk issues in the world. 
											I may post my kaggle competition entries, ML research summaries, 
											my attempts at re-implementing methods of Mechanistic Interpretability.
										</p>
										<p>
											I foresee upskilling first, then be competent enough for SERI MATS, and then going from there. 
										</p>
									</section>
									<section>
										<header>
											<h3>I will most likely start with replicating publications</h3>
										</header>
										<p>
											May update: I believe that I should be able to adopt techniques from SOTA AI research publications and fill in my knowledge gaps in ML. 
											Some steps may be cloning Pytorch starting with minitorch, implement interpretability tools. 
											A higher-level goal is to try out MATL https://arxiv.org/abs/1707.07907 and IMPALA https://arxiv.org/abs/1802.01561
										</p>
									</section>
								</article>
							</div>
						</div>
						<hr />
						<div class="row">
							<article class="col-4 col-12-mobile special">
								<a href="https://github.com/evhub/mesa-optimization/blob/master/post1.md" class="image featured"><img src="images/MesaOptimizer.png" alt="" /></a>
								<header>
									<h3><a href="https://github.com/evhub/mesa-optimization/blob/master/post1.md">Base & Mesa Optimizer Comparison</a></h3>
								</header>
								<p>	
								- The base optimizer is a gradient descent process to create a model, and the model is designed to accomplish some specific task
								</p>
								<p>
								- The mesa optimizer produces a base optimizer that is itself good at optimizing systems 
								</p>
								<p>
								- The mesa-optimizer is different from a subsystem because it is an optimization process, not an agent. 
								</p>
								<p>
								- Mesa-optimization happens when the base optimizer can find a model that exists for the purpose of optimizing another 
								</p>
								<p>
								- Within every optimizer, there are objectives
								</p>
								<p>
								- Unlike the base objective, the mesa-objective is not specified directly by the programmers
								</p>
								<p>
								- Mesa-optimization sometimes leads to mismatch of base and mesa-objectives. This is called misalignment. 
								</p>
								<p>
								- We can call a model generated by the base optimizer as a learned algorithm
								</p>
								<p> 
									<ol>
										<li>Base optimizer is Evolution</li>
										<li>Base objective is caring about certain favorable alleles' frequency in the population</li>
										<li>Mesa-optimizers are human brains </li>
										<li>Mesa-objective is organisms’ behavior—behavior that is not necessarily aligned with evolution. For example, choosing to not have children and thereby decreasing the exchange of favorable alleles' frequency. </li>
									</ol>
									
								</p>
							</article>
							<article class="col-4 col-12-mobile special">
								<a href="#" class="image featured"><img src="images/pic08.jpg" alt="" /></a>
								<header>
									<h3><a href="#">The Alignment Landscape, Learned Content </a></h3>
								</header>
								<p>
								How might white-box methods fit into the Alignment Plan: 
								</p>
									<ol>
										<li> Model internal access during training and deployment </li>
										<li> The promise of AI to empower </li>
									</ol>
								<p>
									Within every research group working on ML models, we can decompose the workforce into such categories:
								</p>
								<ol>
									<li> Data team (paying humans to generate data points) </li>
									<li> Oversight team </li>
									<li> Deployment of SGD-where-RLHF-is-the-algorithm team </li>
								</ol>
								<p>
									RLHF is Reinforcement Learning from Human Feedback, and the problems with baseline RLHF are oversight and catastrophes. Current proposals that have these problems are:
								</p>
								<ol>
									<li> Using AIs to help oversee (oversight) </li>
									<li> Adversarial training (catastrophes) </li>
								</ol>
					
								<p>After reading Holden Karnofsky's post "[How might we align transformative AI if it’s developed very soon?]", we can conclude that the remaining problems for current ML models are: </p>
								<ol>
									<li>Eliciting latent knowledge</li>
									<li>Easier to detect fakes than to produce fakes. For ChatGPT at least, it is difficult to make fake inputs. It is a war between real AI input vs. fake AI input.</li>
								</ol>
								<p>Large Neural Networks may become power-seeking where:</p>
								<ol>
									<li>Fine-tuning LLMs with widely used RLHF goes awry</li>
									<li>Deceptive AI and fewer requirements to prevent itself from being turned off</li>
									<li>Becoming a system where it requires the acquisition of power or resources to gain power</li>
								</ol>
								<p><strong>So what does it mean relative to technical alignment research?</strong><p>
								<ol>
									<li>Interpretability </li>
									<li>Benchmarking </li>
									<li>Process-based supervision (instead of implicit optimization)</li>
									<li>Scalable oversight (supply a reliable reward or training signal to AI systems that are more capable)</li>
									<li>Elicit latent knowledge (how can we incentive a NN to tell us all the facts it knows relevant to a decision?)</li>
								</ol>
								<p>
									We need to look at scaling laws for compute, for data, and for model size more carefully because progress is shocking to professional forecasters. 
									Metaculus predicts for a powerful AI (passing the multi-modal Turing test) within 9 years with a 25% chance. 
									Technical alignment is still sprouting, and it is preparadigmatic--- there is no agreed strategy yet.
									What current alignment approach is the Swiss-Cheese approach--- combine fallible techniques and hope weaknesses will cancel out. 
								</p>
							</article>
							<article class="col-4 col-12-mobile special">
								<a href="#" class="image featured"><img src="images/pic09.jpg" alt="" /></a>
								<header>
									<h3><a href="#">How likely is deceptive alignment in practice? </a></h3>
								</header>
								<p>
									What does ML inductive biases look like?
									<li>High path-dependence: Different training runs can converge to very different models depending on the particular path taken through model space </li>
									<li>Low path-dependence: Similar training processes converge to essentially the same, simple solution, regardless of early training dynamics </li>
								</p>
								<p>
									Deceptive alignment in the high path-dependence world: Suppose our training process is good enough that, for the model to do well, it has to fully understand what we want--- essentially what you get in the limit of doing enough adversarial training. 
								</p>
								<p>
									What does goal attainment look like for models? We ask: 
									<li>How much marginal performance improvement do we get from each step toward the model class?</li>
									<li>How many steps are needed until the model becomes a member of that class?</li>
								</p>
								<p>
									What are the types of alignment? 
									<li>Internal alignment: An internally aligned mesa-optimizer is a robustly aligned mesa-optimizer that has internalized the base objective in its mesa-objective. The internal alignment path starts with a proxy-aligned model, followed by stochastic gradient descent (SGD). </li>
									<li>Corrigible alignment: A corrigibly aligned mesa-optimizer is a robustly aligned mesa-optimizer that has a mesa-objective that “points to” its epistemic model of the base objective.</li>
									<li>
										Deceptive alignment: A deceptively aligned mesa-optimizer is a pseudoaligned mesa-optimizer that has enough information about the base objective to seem more fit from the perspective of the base optimizer than it actually is
										<li>We start with a proxy-aligned model</li>
										<li>In early training, SGD jointly focused on improving the model's understanding of the world along with improving its proxies </li>
										<li>Model learns about the training process from its input data ---> diminishing returns to better pointers seem to imply that this would take many consecutive steps with rapidly diminishing performance improvements</li>
									</li>
	
								</p>

								<p>
									Simplicity analysis -> M = world_model + optimization_procedure + mesa_objective 
								</p>
								<p>
									Deceptively aligned models have to take the extra step of deducing that they should be trying to play along in the training process. The problem is that speed bias seems uncompetitive. Whether in a world of high or low path dependence, SGD will want to make the model deceptive. We must enact intervention that will change the training dynamics to prevent deception.
								</p>
							</article>
			
							<article class="col-4 col-12-mobile special">
								<a href="#" class="image featured"><img src="images/pic09.jpg" alt="" /></a>
								<header>
									<h3><a href="#">Heuristics</a></h3>
								</header>
								<p>
									What is a heuristic technique? 
				
								</p>
								<p>
									An approach to problem-solve by employing a practical method, arriving at a satisfactory solution nonetheless by taking mental shortcuts and easing the cognitive load of making a decision.
								</p>
								<p>
									Although, these practices are not resistant to cognitive biases. We operate on the terms of bounded rationality, or in my own words, conditional rationality. It is conditional in the sense that within computational boundaries and situational urgency, the behavioral strategic selection is a perfected one. 
								</p>
								<p>
									Heuristics converge on human psychology, delving into self-consciousness and fine-tuning, one decision upon another. Some of the most frequently used examples of heuristics are anchoring and adjustment, which is simply giving a lower or higher bound to allow for systematic adjustments and reasonably deviate from the true value. 
	
								</p>

								<p>
									Another would be availability or regency heuristic, according to Investopedia, is an issue where people would be more alarmed with a chance event happening than if wasn't recently occurring. This is a misaligned sentiment because independent events do not happen in consistent behavior, but rather stochastically. The possibilities do not change from availability. 
								</p>
								<p>
									Another component that may contribute to Heuristics is confirmation bias, which is largely dependent on concurrent existent worldviews which confirm or deny the agent's decision-making tendencies. 
								</p>
							</article>
						</div>
					</div>

				</div>

			<!-- Footer -->
				<div id="footer">
					<div class="container">
						<div class="row">
	
						</div>
						<hr />
						<div class="row">
							<div class="col-12">

								<!-- Contact -->
									<section class="contact">
										<header>
											<h3>Have Feedback for Lisa?</h3>
										</header>
										<p><a href = "https://www.admonymous.co/lisawang">Please leave an anonymous feedback.</a></p>
										<ul class="icons">
											<li><a href="https://twitter.com/wanghuiqine" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
											<li><a href="https://github.com/lisahqwang" class="icon brands fa-github"><span class="label">Github</span></a></li>
											<li><a href="https://www.linkedin.com/in/lisahqwang/" class="icon brands fa-linkedin-in"><span class="label">Linkedin</span></a></li>
										</ul>
									</section>

								<!-- Copyright -->
									<div class="copyright">
										<ul class="menu">
											<li>&copy; Lisa Huiqin Wang. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
										</ul>
									</div>

							</div>

						</div>
					</div>
				</div>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
